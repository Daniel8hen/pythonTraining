{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Introduction to Spark MLlib with Python</center>\n",
    "## <center>Clustering</center>\n",
    "### <center>July 20, 2016</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://ibm.box.com/shared/static/wfbduwkbx22nx3i2psbp9g27s2p9s86v.png\", width=\"500\" align = 'center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Welcome to the fourth lab in the course, Machine Learning with Spark MLlib.</b>\n",
    "### <b>Spark has many libraries, namely under MLlib (Machine Learning Library)! Spark allows for quick and easy scalability of practical machine learning!</b>\n",
    "\n",
    "In this lab exercise, you will learn how to create two different clustering algorithms with Spark MLlib! These clustering algorithms include K-Means clustering and Gaussian Mixture Clustering. We will also look at some of the information that the functions can provide about the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Notebook Commands\n",
    "#### In case you haven't dealt with a Jupyter Notebook before, here are some quick, useful commands that may be handy to get started.\n",
    "<ul>\n",
    "    <li>Run a cell: CTRL + ENTER</li>\n",
    "    <li>Create a cell above a cell: a</li>\n",
    "    <li>Create a cell below a cell: b</li>\n",
    "    <li>Change a cell to Markdown: m</li>\n",
    "    \n",
    "    <li>Change a cell to code: y</li>\n",
    "</ul>\n",
    "\n",
    "<b> If you are interested in more keyboard shortcuts, go to Help -> Keyboard Shortcuts </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the following libraries:\n",
    "<ul>\n",
    "    <li>KMeans, KMeansModel from pyspark.mllib.clustering</li>\n",
    "    <li>make_blobs from sklearn.datasets.samples_generator</li>\n",
    "    <li>array from numpy</li>\n",
    "    <li>sqrt from math</li>\n",
    "    <li>numpy as np</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be generating our own data using the make_blobs class, so we will need to create a random seed to initialize a random number generator. Using numpy's random.seed() function, pass it the value <b>0</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create the dataset. Using the make_blobs function will output two <b>numpy.ndarray</b> values, which we will store as <b>X</b> and <b>y</b>. The <b>make_blobs</b> function accepts three inputs:\n",
    "<ul>\n",
    "    <li>1st: <b>n_samples</b>: The number of samples you want to generate. (We will use 6500 samples)</li>\n",
    "    <li>2nd: <b>centers</b>: A list of lists containing the centers that we want the clusters to center around. The centers look like: [x_value, y_value]. (We will be creating three centers at the following coordinates: [1,2], [3,-1], and [-4,-2])</li>\n",
    "    <li>3rd: <b>cluster_std</b>: The standard deviation of the clusters, which determines how spread out the clusters are. (We will use 1.1)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=6500, centers=[[1,2],[3,-1],[-4,2]], cluster_std=1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first ndarray, <b>X</b> contains the actual samples themselves and the second ndarray, <b>y</b> contains the labels for the samples. We will be using <b>X</b> as the data to train the KMeans model. This means we will need to convert the data from a <b>ndarray</b> to a <b>RDD</b>. Do this using <b>sc.parallelize</b> and passing in <b>X</b>, calling the output <b>kmeans_data</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = SparkContext() for first run\n",
    "kmeans_data = sc.parallelize(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create the KMeans model called <b>kmeans_model</b> using <b>KMeans.train</b> and passing in the following parameters:\n",
    "<ul>\n",
    "    <li>1st: The data (use kmeans_data)</li>\n",
    "    <li>2nd: The number of desired clusters (use 3)</li>\n",
    "    <li>3rd: The max number of iterations (use maxIterations=10)</li>\n",
    "    <li>4th: The number of runs (use runs=10)</li>\n",
    "    <li>5th: The initialization mode (use initializationMode=\"k-means||\")</li>\n",
    "    <li>6th: The initialization steps (use initializationSteps=5)</li>\n",
    "    <li>7th: The value for epsilon (use epsilon=0.003)</li>\n",
    "    <li>8th: The initial model (use initialModel=None)</li>\n",
    "</ul> <br>\n",
    "Note: You may get a deprecation warning, please ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/pyspark/mllib/clustering.py:347: UserWarning: The param `runs` has no effect since Spark 2.0.0.\n",
      "  warnings.warn(\"The param `runs` has no effect since Spark 2.0.0.\")\n"
     ]
    }
   ],
   "source": [
    "kmeans_model = KMeans.train(kmeans_data, 3, maxIterations=10, runs=10, initializationMode=\"k-means||\",initializationSteps=5,epsilon=0.003,initialModel=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define a function that will evaluate the clustering by computing the Within Set Sum of Squared Error. <br><br>\n",
    "\n",
    "Within Set Sum of Squared errors will find take the difference between all of the points to its cluster, square each difference, sum all of those results together, then square root it. You can see that in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(model, point):\n",
    "    center = model.centers[model.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to take the data (<b>kmeans_data</b>) and map it to the error function (<b>.map(lambda p: error(p))</b>). After, we add up the result of each cluster together using a .reduce function (<b>.reduce(lambda x, y: x + y)</b>). The Within Set Sum of Squared Error result will be stored in a variable called <b>WSSSE</b>.<br> <br>\n",
    "\n",
    "Then you can print out the <b>WSSSE</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8706.90278674\n"
     ]
    }
   ],
   "source": [
    "WSSSE = kmeans_data.map(lambda p: error(kmeans_model, p)).reduce(lambda x, y: x+y)\n",
    "print(WSSSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have built the K-Means model, let's look at some of the functions that it comes with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting off, let's take a look at the number of clusters in the model. This can be done by calling <b>.k</b> on <b>kmeans_model</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_model.k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 3 clusters, which is expected since that's how many clusters we initialized when training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can take a look at the coordinates of where the cluster centers actually are. Call <b>.clusterCenters</b> on <b>kmeans_model</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 3.02182138, -1.04459141]),\n",
       " array([-4.02437674,  1.9578032 ]),\n",
       " array([0.91902958, 2.04128787])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_model.clusterCenters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here, we get a list of 3 arrays, each containing the x and y coordinates of each cluster center!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try predicting one of the points from the dataset we created, <b>X</b>. First, let's check the <b>shape</b> of </b>X</b> with .shape. The shape should be (6500, 2), or 6500 data points, each with a x and y value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6500, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take we will use <b>.predict</b> to predict one of the data points from <b>X</b>, so we can see what cluster that point belongs to. We can index <b>X</b> to get a single data point (ex. X[7]). So try something like <b>kmeans_model.predict(X[7])</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_model.predict(X[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look back at the make_blobs function we used. There are two inputs, which are <b>X</b> that has the coordinates of each data point and <b>y</b> that has the label for each data point. The label is the cluster where the data point was generated from! So knowing this, we can see if the model clustered the data point under the same cluster as it was made from. <br>\n",
    "We can run <b>kmeans_model.predict(X[7]) == y[7]</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_model.predict(X[18]) == y[18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the coordinates of the data point (ex. <b>print(X[7])</b>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.11109986 2.0942723 ]\n"
     ]
    }
   ],
   "source": [
    "print(X[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the <b>x</b> and <b>y</b> coordinates of the data points. Since k-means uses euclidean distance, you can check the coordinates of this point with each cluster center to make sure it is correct if you are curious! <br> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that you can insert a list containing a x distance and y distance and the model will determine which cluster that data point belongs to. <br>(ex. kmeans_model.predict([5, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_model.predict([10,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see the computation cost of the entire kmeans_data dataset by calling <b>.computeCost</b> on <b>kmeans_model</b>, passing in <b>kmeans_data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14677.975272676682"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_model.computeCost(kmeans_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now move into creating a Gaussian Mixture Clustering Model and looking at its functions as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the following libraries:\n",
    "<ul>\n",
    "    <li>GaussianMixture from pyspark.mllib.clustering</li>\n",
    "    <li>array from numpy</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from pyspark.mllib.clustering import GaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the dataset, we will be creating a small dataset, as it will be easier to visually see the predictions of each point. <br><br>\n",
    "\n",
    "First, we will create a numpy array called <b>data</b>, which will contain the data. We can create a <b>numpy array</b> by passing in a <b>list</b>. The list will contain the following values: <b>4,5, -3,1, 9,5, 1,-2, 7,2, -10,-3, 4,7</b>. Outside of <b>array()</b>, we will use <b>.reshape(7, 2)</b> which will give us 7 data points, where the list values correspond to the x and y coordinates. <br>(ex. for 1 data point: array([1,2]).reshape(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = array([4,5, -3,1, 9,5, 1,-2, 7,2, -10,-3, 4,7]).reshape(7,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train our model, we need to turn the data into a RDD. We can run SparkContext's <b>.parallelize</b>, passing in the <b>numpy array</b> or <b>data</b> as input. Call the output <b>gm_data</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_data = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the Gaussian Mixture Model called <b>gm_model</b>, using <b>GaussianMixture.train</b>, passing in the following parameters:\n",
    "<ul>\n",
    "    <li>1st: The training data (use gm_data)</li>\n",
    "    <li>2nd: The number of desired clusters (used 3)</li>\n",
    "    <li>3rd: convergenceTol (use convergenceTol=0.0006)</li>\n",
    "    <li>4th: maxIterations (use maxIterations=50)</li>\n",
    "    <li>5th: initialModel (use initialModel=None)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_model = GaussianMixture.train(gm_data, 3, convergenceTol=0.0006, maxIterations=50, initialModel=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have built the Gaussian Mixture Model, let's try out some functions on it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start off by seeing how much clusters it has, with <b>.k</b> on <b>gm_model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm_model.k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be 3 clusters as the output, which is the value we used to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at what each Gaussian looks like with <b>gm_model.gaussians</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MultivariateGaussian(mu=DenseVector([1.0, -2.0]), sigma=DenseMatrix(2, 2, [0.0, 0.0, 0.0, 0.0], 0)),\n",
       " MultivariateGaussian(mu=DenseVector([6.6667, 4.6667]), sigma=DenseMatrix(2, 2, [4.2222, -2.1111, -2.1111, 4.2222], 0)),\n",
       " MultivariateGaussian(mu=DenseVector([-3.0, 1.0]), sigma=DenseMatrix(2, 2, [32.6667, 18.6667, 18.6667, 10.6667], 0))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm_model.gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this output, you can get a feel for the size of each of these gaussians by looking at the <b>mean (mu)</b> and the <b>standard deviation (sigma)</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the <b>weight</b> of each gaussian in the mixture in order to see which gaussians make up most of the mixture. Run <b> gm_model.weights</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14285714, 0.42857153, 0.42857132])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm_model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to use <b>.predict</b> to see what each of the data points that were used to train the model are classified as. Use <b>gm_data</b> as input into <b>.predict</b>. Outside of .predict, add on <b>.collect()</b> so we can see the output in list form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 1, 0, 1, 2, 1]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm_model.predict(gm_data).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try using <b>.predictSoft</b> on <b>gm_data</b> to see what that output looks like. Remember that predictSoft provides each gaussians membership to each datapoint, showing which gaussian has the most precedence. Don't forget a <b>.collect()</b> afterwards to display the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array('d', [2.3881200540782028e-20, 7.380631194279263e-07, 0.9999992619368806]),\n",
       " array('d', [1.1280683480311063e-20, 1.6462747447581932e-16, 0.9999999999999998]),\n",
       " array('d', [3.235661529552778e-14, 0.9999999999999353, 3.235661529552778e-14]),\n",
       " array('d', [1.0, 2.5658492254156683e-20, 2.1137637929561462e-26]),\n",
       " array('d', [3.235660892452372e-14, 0.9999999999999353, 3.235660892452372e-14]),\n",
       " array('d', [2.38812004843812e-20, 2.38812004843812e-20, 1.0]),\n",
       " array('d', [3.235659648115988e-14, 0.9999999999999353, 3.235659648115988e-14])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm_model.predictSoft(gm_data).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for the highest value in each data points and see if that aligns with the output from <b>.predict</b>, which it should. .predictSoft provides more information on the values associated with .predict."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
